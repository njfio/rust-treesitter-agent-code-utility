{
  "default_provider": "OpenAI",
  "providers": {
    "OpenAI": {
      "enabled": true,
      "api_key": "${OPENAI_API_KEY}",
      "base_url": "https://api.openai.com/v1",
      "organization": null,
      "models": [
        {
          "name": "gpt-4",
          "context_length": 8192,
          "max_tokens": 4096,
          "supports_streaming": true,
          "cost_per_token": 0.00003,
          "supported_features": [
            "CodeExplanation",
            "SecurityAnalysis",
            "RefactoringSuggestions",
            "ArchitecturalInsights",
            "PatternDetection",
            "QualityAssessment",
            "DocumentationGeneration",
            "TestGeneration"
          ]
        },
        {
          "name": "gpt-3.5-turbo",
          "context_length": 4096,
          "max_tokens": 2048,
          "supports_streaming": true,
          "cost_per_token": 0.000002,
          "supported_features": [
            "CodeExplanation",
            "DocumentationGeneration",
            "TestGeneration"
          ]
        }
      ],
      "default_model": "gpt-4",
      "timeout": "30s",
      "rate_limit": {
        "requests_per_minute": 60,
        "tokens_per_minute": 100000,
        "burst_size": 10
      },
      "retry": {
        "max_retries": 3,
        "initial_delay": "1s",
        "max_delay": "30s",
        "backoff_multiplier": 2.0
      }
    },
    "Anthropic": {
      "enabled": false,
      "api_key": "${ANTHROPIC_API_KEY}",
      "base_url": "https://api.anthropic.com/v1",
      "models": [
        {
          "name": "claude-3-sonnet-20240229",
          "context_length": 200000,
          "max_tokens": 4096,
          "supports_streaming": true,
          "cost_per_token": 0.000015,
          "supported_features": [
            "CodeExplanation",
            "SecurityAnalysis",
            "RefactoringSuggestions",
            "ArchitecturalInsights"
          ]
        }
      ],
      "default_model": "claude-3-sonnet-20240229",
      "timeout": "30s",
      "rate_limit": {
        "requests_per_minute": 50,
        "tokens_per_minute": 80000,
        "burst_size": 5
      },
      "retry": {
        "max_retries": 3,
        "initial_delay": "1s",
        "max_delay": "30s",
        "backoff_multiplier": 2.0
      }
    },
    "Local": {
      "enabled": false,
      "api_key": null,
      "base_url": "http://localhost:8080",
      "models": [
        {
          "name": "local-model",
          "context_length": 4096,
          "max_tokens": 2048,
          "supports_streaming": false,
          "cost_per_token": 0.0,
          "supported_features": [
            "CodeExplanation"
          ]
        }
      ],
      "default_model": "local-model",
      "timeout": "60s",
      "rate_limit": {
        "requests_per_minute": 1000,
        "tokens_per_minute": 1000000,
        "burst_size": 100
      },
      "retry": {
        "max_retries": 2,
        "initial_delay": "500ms",
        "max_delay": "10s",
        "backoff_multiplier": 1.5
      }
    }
  },
  "features": {
    "code_explanation": {
      "enabled": true,
      "preferred_provider": "OpenAI",
      "preferred_model": "gpt-4",
      "temperature": 0.3,
      "max_tokens": 2048,
      "use_cache": true,
      "cache_ttl": 3600
    },
    "security_analysis": {
      "enabled": true,
      "preferred_provider": "OpenAI",
      "preferred_model": "gpt-4",
      "temperature": 0.1,
      "max_tokens": 3072,
      "use_cache": true,
      "cache_ttl": 1800
    },
    "refactoring_suggestions": {
      "enabled": true,
      "preferred_provider": "OpenAI",
      "preferred_model": "gpt-4",
      "temperature": 0.5,
      "max_tokens": 2048,
      "use_cache": true,
      "cache_ttl": 3600
    },
    "architectural_insights": {
      "enabled": true,
      "preferred_provider": "OpenAI",
      "preferred_model": "gpt-4",
      "temperature": 0.4,
      "max_tokens": 3072,
      "use_cache": true,
      "cache_ttl": 7200
    },
    "pattern_detection": {
      "enabled": true,
      "preferred_provider": null,
      "preferred_model": null,
      "temperature": 0.2,
      "max_tokens": 1536,
      "use_cache": true,
      "cache_ttl": 3600
    },
    "quality_assessment": {
      "enabled": true,
      "preferred_provider": null,
      "preferred_model": null,
      "temperature": 0.3,
      "max_tokens": 2048,
      "use_cache": true,
      "cache_ttl": 3600
    },
    "documentation_generation": {
      "enabled": true,
      "preferred_provider": "OpenAI",
      "preferred_model": "gpt-3.5-turbo",
      "temperature": 0.4,
      "max_tokens": 1536,
      "use_cache": true,
      "cache_ttl": 7200
    },
    "test_generation": {
      "enabled": true,
      "preferred_provider": "OpenAI",
      "preferred_model": "gpt-3.5-turbo",
      "temperature": 0.6,
      "max_tokens": 2048,
      "use_cache": true,
      "cache_ttl": 3600
    }
  },
  "global": {
    "enable_cache": true,
    "cache": {
      "cache_type": "memory",
      "max_size_mb": 100,
      "default_ttl": 3600,
      "key_prefix": "ai_cache",
      "redis_url": null,
      "file_cache_dir": null
    },
    "enable_metrics": true,
    "log_level": "info",
    "max_concurrent_requests": 10
  }
}
